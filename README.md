# Gender Biases Unexpectedly Fluctuate in the Pre-training Stage of Masked Language Models

This repository includes the supplementary code and data for [this paper](https://arxiv.org/abs/2211.14639).

## Data

We provide the following data files in the `data` folder:

- `professions_prompts.json` is the original list of 893 professions from [this repository](https://github.com/aliciasun/natural-prompts).
- `*-professions.txt` are the lists of professions.
- `2000-1700-total.txt` is the yearly sizes of the English BookCorpus, copied from the [supplementary spreadsheet](https://www.science.org/doi/suppl/10.1126/science.1199644/suppl_file/michel.som.revision.2.som_data.xlsx) of [this paper](https://www.science.org/doi/full/10.1126/science.1199644).
- `ngram-freqs.jsonl` is the yearly frequencies of professions from 1500 to 2019.
- `*-prior.pkl` contain the prior probabilies estimated from checkpoints.
- `roberta-professions.pkl` contains the contains the pronoun probabilites from RoBERTa checkpoints, including both the single public checkpoint and ones from the replicated pre-training run.
- `bert-professions-seed-*.pkl` similarly contains the pronoun probabilites from BERT checkpoints. There is one separate file for each seed. 

## Code

Due to resource restrictions, we have run all our code on Google Colab in the form of Jupyter Notebooks. 

### Prerequisites

The intermediate checkpoints of [RoBERTa](https://github.com/leo-liuzy/probe-across-time) and [BERT](https://console.cloud.google.com/storage/browser/multiberts) should be downloaded. The BERT checkpoints should be further converted into the PyTorch format by using `transformers-cli convert`. See `source/pytorch-checkpoint-conversion.ipynb` for reference.

### Data Processing

All notebooks in the `source` folder have been commented in detail.

### Figures

The figures are generated by `source/plot-new.ipynb` and are saved in the `figures` folder. Files are named according to the relevant parameters as explained in the paper.